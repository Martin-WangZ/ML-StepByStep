{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ID3 Implement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for numeric data (need coding data to number).\n",
    "class ID3v1():\n",
    "    def __init__(self,x,maxDepth=3,minGain=0.5):\n",
    "        self.__maxDepth=maxDepth\n",
    "        self.__minGain=minGain\n",
    "        self.__depth=0\n",
    "        self.__tree=[]\n",
    "        self.__features=np.arrage(x.shape[1])\n",
    "\n",
    "    # given the y in that dataset , compute the entropy\n",
    "    def _entropy(self,y):\n",
    "        values,counts=np.unique(y,return_counts=True)\n",
    "        total=len(y)\n",
    "        entropy=-np.sum([count/total*np.log2(count/total) for count in counts])\n",
    "        return entropy\n",
    "    # given attribute index and that dataset(x,y), compute conditional entropy\n",
    "    def _conditEntropy(self,attribute,x,y):\n",
    "        values=np.unique(x[:,attribute])\n",
    "        entropy=[]\n",
    "        # iterate each value in that attribute\n",
    "        for value in values:\n",
    "            index=np.argwhere(value==x[:,attribute]).reshape(-1)\n",
    "            # how many data points belongs to that attribute value\n",
    "            label=y[index]\n",
    "            weight=len(label)/len(y)\n",
    "            h=self._entropy(label)\n",
    "            attrEntrop=weight*h\n",
    "            entropy.append(attrEntrop)\n",
    "        conditEntropy=np.sum(entropy)\n",
    "        return conditEntropy\n",
    "\n",
    "    def id3Train(self,x,y,):\n",
    "        # if all data belong to one class, then return\n",
    "        values,counts=np.unique(y,return_counts=True)\n",
    "        # if only a class, return the tree\n",
    "        if len(values)==1:\n",
    "            # depth and its class\n",
    "            if self.__depth==0:\n",
    "                #(tree depth, feature, value, class)\n",
    "                self.__tree.append((self.__depth,'Root Node',None,values.item()))\n",
    "            return self.__tree\n",
    "        # if no features, return, # if it is root, it is impossible the length of feature is 0\n",
    "        elif len(self.__features)==0 or self.__depth>self.__maxDepth:\n",
    "            return self.__tree\n",
    "        else:\n",
    "            # start building decision tree\n",
    "            if self.__depth==0:\n",
    "                valMaxCount=values[np.argmax(counts)]\n",
    "                self.__tree.append((self.__depth,'Root Node',None,valMaxCount))\n",
    "            # 1. compute data entropy\n",
    "            h=self._entropy(y)\n",
    "            # 2. compute conditional entropy of each feature\n",
    "            conEL=[]\n",
    "            for feature in self.__features:\n",
    "                conditEntropy=self._conditEntropy(feature,x,y)\n",
    "                conEL.append(conditEntropy)\n",
    "            # 3. compute gain\n",
    "            gain=h-np.array(conEL)\n",
    "            # 4. max gain\n",
    "            maxGain=np.max(gain)\n",
    "            # if max gain less than threshold\n",
    "            if maxGain<=self.__minGain:\n",
    "                return self.__tree\n",
    "            # else split the to sub-nodes\n",
    "            else:\n",
    "                # 5. get feature with max gain and set it as standard to split data\n",
    "                maxGainF=self.__features[np.argmax(gain)]\n",
    "                # 6. according to the feature values to split dataset\n",
    "                attrValues=np.unique(x[:,maxGainF])\n",
    "                # a little tricky\n",
    "                loop=self.__depth\n",
    "                preXsub,preYsub,subxs,subys=None,None,None,None\n",
    "                for k,attrValue in enumerate(attrValues):\n",
    "                    xAttrIndex=np.argwhere(attrValue==x[:,maxGainF])\n",
    "                    xsub=x[xAttrIndex,:]\n",
    "                    ysub=y[xAttrIndex]\n",
    "                    if k>0:\n",
    "                        subxs=np.vstack((preXsub,xsub))\n",
    "                        subys=np.vstack((preYsub,ysub))\n",
    "                    preXsub=xsub\n",
    "                    preYsub=ysub\n",
    "                    subvals,subcts=np.unique(ysub,return_counts=True)\n",
    "                    val=subvals[np.argmax(subcts)]\n",
    "                    self.__tree.append((loop+1,maxGainF,attrValue,val))\n",
    "                # depth + 1 (important! depth increase must be after the loop above)\n",
    "                self.__depth+=1\n",
    "                if self.__depth>self.__maxDepth:\n",
    "                    return self.__tree\n",
    "                else:\n",
    "                    # after built siblings, remove the feature to ready to next loop\n",
    "                    for subX,subY in zip(subxs,subys):\n",
    "                        leftF=np.argwhere(maxGainF!=self.__features)\n",
    "                        self.__features=np.array(self.__features)[leftF].tolist()\n",
    "                        return self.id3Train(subX,subY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text data\n",
    "class ID3v2:\n",
    "\n",
    "    # define entropy\n",
    "    @classmethod\n",
    "    def entropy(cls, target_col):\n",
    "        elements, counts = np.unique(target_col, return_counts=True)\n",
    "        entropy = -np.sum(\n",
    "            [(counts[i] / np.sum(counts)) * np.log2(counts[i] / np.sum(counts)) for i in range(len(elements))])\n",
    "        return entropy\n",
    "\n",
    "    # define information gain based on given split attribute\n",
    "    @classmethod\n",
    "    def infor_gain(cls, data, split_attribute_name):\n",
    "        target_name = data.columns[-1]\n",
    "        data_entropy = cls.entropy(data[target_name])\n",
    "        elements, counts = np.unique(data[split_attribute_name], return_counts=True)\n",
    "        entropy_split_attribute = np.sum([(counts[i] / np.sum(counts)) * cls.entropy(\n",
    "            data.where(data[split_attribute_name] == elements[i]).dropna()[target_name]) for i in range(len(elements))])\n",
    "        information_gain = data_entropy - entropy_split_attribute\n",
    "        return information_gain\n",
    "\n",
    "    # choose the attribute with the biggest information gain to split the dataset\n",
    "    @classmethod\n",
    "    def select_best_attribute(cls, data):\n",
    "        attributes = data.columns[:-1]\n",
    "        infor_gain_list = []\n",
    "        for attribute in attributes:\n",
    "            information_gain = cls.infor_gain(data, attribute)\n",
    "            infor_gain_list.append((information_gain, attribute))\n",
    "\n",
    "        return max(infor_gain_list)\n",
    "\n",
    "    # get tree depth through filtering the nodes named with distinct values of these attributes\n",
    "    @classmethod\n",
    "    def tree_depth(cls, data, decision_tree):\n",
    "        # node_names= features + ['yes','no']\n",
    "        node_names = list(data.columns[:-1]) + ['yes', 'no']\n",
    "        # if leaf node, just add 1 to tree depth\n",
    "        if decision_tree == 'yes' or 'no':\n",
    "            return 1\n",
    "        # otherwise continue searching in the tree to find the longest path with the given node names\n",
    "        else:\n",
    "            return max(cls.tree_depth(data, v) if all((isinstance(v, dict), k in node_names)) else 0 for k, v in\n",
    "                       decision_tree.items()) + 1\n",
    "\n",
    "    # id3 algorithm\n",
    "    def id3(self, data, previous_data, attributes, max_tree_depth=5, threshold=0.1, parent_node_label=None):\n",
    "        target_name = data.columns[-1]\n",
    "        # if data is empty, return it's parent label\n",
    "        if len(data) == 0:\n",
    "            return parent_node_label\n",
    "        # if all data records belong to one class, return that label\n",
    "        elif len(np.unique(data[target_name])) == 1:\n",
    "            label = np.unique(data[target_name])[0]\n",
    "            return label\n",
    "        # if the feature is empty, return its parent label\n",
    "        elif len(attributes) == 0:\n",
    "            return parent_node_label\n",
    "        # else calculate information gain, choose the biggest gain to split dataset into subset\n",
    "        else:\n",
    "            # before splitting, records the current node label as parent label\n",
    "            parent_node_label = np.unique(data[target_name])[\n",
    "                np.argmax(np.unique(data[target_name], return_counts=True)[1])]\n",
    "            # get the biggest information gain and corresponding attribute\n",
    "            best_information_gain, best_attribute = self.select_best_attribute(data)\n",
    "            # if the information gain less than threshold, then return its parent node label\n",
    "            if best_information_gain <= threshold:\n",
    "                return parent_node_label\n",
    "            # split dataset according to the attribute value\n",
    "            else:\n",
    "                # create decision tree. it also can be regarded as subtree structure\n",
    "                decision_tree = {best_attribute: {}}\n",
    "                # remove the splitting attribute first so next loop we will not consider this attribute again\n",
    "                attributes = [attribute for attribute in attributes if attribute != best_attribute]\n",
    "                # add nodes at same level on decision tree\n",
    "                for value in np.unique(data[best_attribute]):\n",
    "                    # get corresponding subset according to distinct attribute value\n",
    "                    sub_data = data.where(data[best_attribute] == value).dropna()\n",
    "                    # now sub_data is new dataset and the original dataset becomes previous one. set parent_node_label as current node label.\n",
    "                    sub_tree = self.id3(sub_data, data, attributes, parent_node_label=parent_node_label)\n",
    "\n",
    "                    # if tree depth is over the max depth, then stop growing our decision tree\n",
    "                    if self.tree_depth(sub_data, sub_tree) >= max_tree_depth:\n",
    "                        break\n",
    "                    # otherwise, grow our decision tree\n",
    "                    else:\n",
    "                        # decision_tree[best_attribute] means all subtrees but decision_tree[best_attribute][value] means choosing one of subtrees\n",
    "                        decision_tree[best_attribute][value] = sub_tree\n",
    "                # after building subtree in each node, return the final tree(including subtree)\n",
    "                return decision_tree\n",
    "\n",
    "    # predict the label of a record in dataset\n",
    "    @classmethod\n",
    "    def fit_one_record(cls, row, decision_tree):\n",
    "        # row.keys()=attribute names\n",
    "        for key in list(row.keys()):\n",
    "            if key in list(decision_tree.keys()):\n",
    "                label = decision_tree[key][row[key]]\n",
    "                # if it's still dict, do it again\n",
    "                if isinstance(label, dict):\n",
    "                    return cls.fit_one_record(row, label)\n",
    "                # else return the label of the record\n",
    "                else:\n",
    "                    return label\n",
    "\n",
    "    # predict all records according to their features\n",
    "    def fit(self, data, decision_tree):\n",
    "        row_dict = data.iloc[:, :-1].to_dict(orient='records')\n",
    "        predicted = pd.DataFrame(columns=['predicted'])\n",
    "        for row_index in range(len(data)):\n",
    "            predicted.loc[row_index, 'predicted'] = self.fit_one_record(row_dict[row_index], decision_tree)\n",
    "        return predicted['predicted']\n",
    "\n",
    "    # get data accuracy through searching the corresponding attribute value in the decision tree.\n",
    "    def accuracy(self, data, decision_tree):\n",
    "        predicted = self.fit(data, decision_tree)\n",
    "        accuracy_score = np.sum(predicted == data.iloc[:, -1]) / len(data)\n",
    "        return accuracy_score\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (Machine-Learning)",
   "language": "python",
   "name": "pycharm-b417c3b3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}