{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# C4.5 Algorithm Implement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3: information gain \n",
    "## C4.5: ratio of information gain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# for numeric data (need encoding data to number).\n",
    "class C45:\n",
    "\n",
    "    def __init__(self, maxDepth=3, minGain=0):\n",
    "        self.__maxDepth = maxDepth\n",
    "        self.__minGain = minGain\n",
    "        self.__tree = []\n",
    "\n",
    "    # given the y in that dataset , compute the entropy\n",
    "    def _entropy(self, y):\n",
    "        values, counts = np.unique(y, return_counts=True)\n",
    "        total = len(y)\n",
    "        entropy = -np.sum([count / total * np.log2(count / total) for count in counts])\n",
    "        return entropy\n",
    "\n",
    "    # given attribute index and that dataset(x,y), compute conditional entropy\n",
    "    def _conditEntropy(self, attribute, x, y):\n",
    "        values = np.unique(x[:, attribute])\n",
    "        entropy = []\n",
    "        # iterate each value in that attribute\n",
    "        for value in values:\n",
    "            index = np.nonzero(value == x[:, attribute])\n",
    "            # how many data points belongs to that attribute value\n",
    "            label = y[index]\n",
    "            weight = len(label) / len(y)\n",
    "            h = self._entropy(label)\n",
    "            attrEntrop = weight * h\n",
    "            entropy.append(attrEntrop)\n",
    "        conditEntropy = np.sum(entropy)\n",
    "        return conditEntropy\n",
    "\n",
    "    def C45Train(self, x, y, depth=0, features=None):\n",
    "        if np.any(features) == None:\n",
    "            features = np.arange(x.shape[1])\n",
    "        # if no data return the tree\n",
    "        if len(y) == 0:\n",
    "            return self.__tree\n",
    "        # if all data belong to one class, then return\n",
    "        values, counts = np.unique(y, return_counts=True)\n",
    "        # if only a class, return the tree\n",
    "        if len(values) == 1:\n",
    "            # depth and its class\n",
    "            if depth == 0:\n",
    "                #(tree depth, feature, value, class)\n",
    "                self.__tree.append((depth, 'Root Node', None, values.item()))\n",
    "            return self.__tree\n",
    "        # if no features, return, # if it is root, it is impossible the length of feature is 0\n",
    "        elif len(features) == 0 or depth >= self.__maxDepth:\n",
    "            return self.__tree\n",
    "        else:\n",
    "            # start building decision tree\n",
    "            if depth == 0:\n",
    "                valMaxCount = values[np.argmax(counts)]\n",
    "                self.__tree.append((depth, 'Root Node', None, valMaxCount))\n",
    "            # 1. compute data entropy\n",
    "            h = self._entropy(y)\n",
    "            # 2. compute conditional entropy of each feature\n",
    "            conEL = []\n",
    "            for feature in features:\n",
    "                conditEntropy = self._conditEntropy(feature, x, y)\n",
    "                conEL.append(conditEntropy)\n",
    "            # 3. compute gain\n",
    "            gain = h - np.array(conEL)\n",
    "            # add this part and change the criterion variable with infGain\n",
    "            #==============* c4.5=============\n",
    "            infGain=[]\n",
    "            for i,feature in enumerate(features):\n",
    "                hf = self._entropy(x[:, feature])\n",
    "                # if hf=0 => H(D|A)=H(D) => gain=0 => infGain=0\n",
    "                if hf==0:\n",
    "                    infGainE=0\n",
    "                else:\n",
    "                    infGainE=gain[i]/hf\n",
    "                infGain.append(infGainE)\n",
    "            #==============* c4.5=============\n",
    "            # 4. max gain\n",
    "            maxGain = np.max(infGain)  # changed\n",
    "            # if max gain less than threshold\n",
    "            if maxGain <= self.__minGain:\n",
    "                return self.__tree\n",
    "            # else split the to sub-nodes\n",
    "            else:\n",
    "                # 5. get feature with max gain and set it as standard to split data\n",
    "                maxGainF = features[np.argmax(infGain)]  # changed\n",
    "                # 6. according to the feature values to split dataset\n",
    "                attrValues = np.unique(x[:, maxGainF])\n",
    "                depth += 1\n",
    "                tree = None\n",
    "                for attrValue in attrValues:\n",
    "                    xAttrIndex = np.nonzero(attrValue == x[:, maxGainF])\n",
    "                    xsub = x[xAttrIndex]\n",
    "                    ysub = y[xAttrIndex]\n",
    "                    subvals, subcts = np.unique(ysub, return_counts=True)\n",
    "                    val = subvals[np.argmax(subcts)]\n",
    "                    self.__tree.append((depth, maxGainF, attrValue, val))\n",
    "                    leftFeatures = features[np.nonzero(maxGainF != features)]\n",
    "                    tree = self.C45Train(xsub, ysub, depth,leftFeatures)\n",
    "                # tree = self.__tree\n",
    "                return tree\n",
    "\n",
    "    # Node structure: (tree depth,feature,value,class)\n",
    "    def predict(self, x):\n",
    "        predL = []\n",
    "        # for each data points\n",
    "        for data in x:\n",
    "            # a data point has match history in the tree\n",
    "            matchL = []\n",
    "            for attribute, value in enumerate(data):\n",
    "                # iterative tree\n",
    "                for node in self.__tree:\n",
    "                    # if match any tree node, add that node to match history\n",
    "                    if attribute == node[1]:\n",
    "                        if value == node[2]:\n",
    "                            matchL.append((node[0], node[3]))\n",
    "                        else:\n",
    "                            continue\n",
    "                    else:\n",
    "                        continue\n",
    "            # find class with max tree depth as the final prediction\n",
    "            pred = max(matchL, key=lambda ele: ele[0])[1]\n",
    "            predL.append(pred)\n",
    "        return predL\n",
    "\n",
    "    def accuracy(self, x, y):\n",
    "        predL = self.predict(x)\n",
    "        return np.mean(y == predL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate history data\n",
    "np.random.seed(42)\n",
    "hx=np.random.randint(10,size=(100,5))\n",
    "hy=np.random.choice(np.arange(3),size=100,replace=True)\n",
    "\n",
    "# generate test data\n",
    "np.random.seed(0)\n",
    "tx=np.random.randint(10,size=(10,5))\n",
    "ty=np.random.choice(np.arange(3),size=10,replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree=[(0, 'Root Node', None, 0), (1, 2, 0, 0), (2, 0, 1, 0), (2, 0, 4, 1), (2, 0, 6, 0), (3, 1, 1, 0), (3, 1, 4, 1), (2, 0, 7, 2), (2, 0, 8, 0), (1, 2, 1, 0), (2, 1, 0, 2), (2, 1, 3, 1), (2, 1, 4, 0), (2, 1, 5, 0), (2, 1, 6, 0), (2, 1, 9, 0), (1, 2, 2, 0), (2, 1, 0, 0), (2, 1, 1, 1), (2, 1, 3, 2), (2, 1, 5, 0), (2, 1, 6, 0), (2, 1, 7, 2), (2, 1, 8, 0), (2, 1, 9, 0), (1, 2, 3, 0), (2, 1, 0, 0), (3, 0, 2, 0), (3, 0, 3, 2), (2, 1, 1, 2), (2, 1, 2, 1), (2, 1, 4, 0), (2, 1, 5, 0), (2, 1, 8, 0), (2, 1, 9, 1), (1, 2, 4, 2), (2, 3, 0, 1), (3, 0, 2, 1), (3, 0, 5, 1), (3, 0, 7, 0), (2, 3, 2, 0), (3, 0, 0, 0), (3, 0, 8, 1), (2, 3, 3, 2), (2, 3, 4, 0), (2, 3, 5, 0), (2, 3, 6, 2), (3, 0, 0, 1), (3, 0, 3, 2), (2, 3, 8, 2), (2, 3, 9, 2), (1, 2, 5, 1), (2, 1, 1, 1), (3, 0, 0, 1), (3, 0, 1, 1), (3, 0, 3, 0), (2, 1, 3, 0), (2, 1, 4, 2), (2, 1, 5, 1), (2, 1, 6, 2), (2, 1, 9, 2), (1, 2, 6, 1), (2, 1, 0, 2), (2, 1, 2, 1), (2, 1, 3, 0), (2, 1, 6, 0), (2, 1, 7, 2), (2, 1, 8, 0), (3, 0, 4, 0), (3, 0, 9, 1), (2, 1, 9, 2), (1, 2, 7, 0), (2, 3, 0, 1), (2, 3, 2, 0), (2, 3, 3, 1), (2, 3, 4, 0), (2, 3, 5, 0), (2, 3, 6, 2), (3, 0, 0, 2), (3, 0, 3, 1), (3, 0, 9, 2), (2, 3, 7, 0), (1, 2, 8, 0), (2, 3, 0, 2), (2, 3, 1, 2), (2, 3, 3, 0), (2, 3, 4, 0), (2, 3, 5, 2), (2, 3, 6, 2), (2, 3, 7, 0), (2, 3, 8, 0), (2, 3, 9, 0), (1, 2, 9, 2), (2, 0, 1, 2), (2, 0, 3, 1), (2, 0, 4, 1), (2, 0, 5, 2), (2, 0, 6, 2), (3, 1, 0, 1), (3, 1, 2, 2), (3, 1, 3, 2), (2, 0, 8, 2), (2, 0, 9, 2)]\n",
      "predicts=[1, 1, 0, 0, 1, 2, 0, 2, 1, 2]\n",
      "accuray=0.1\n"
     ]
    }
   ],
   "source": [
    "dt=C45()\n",
    "tree=dt.C45Train(hx,hy)\n",
    "predicts=dt.predict(tx)\n",
    "accuracy=dt.accuracy(tx,ty)\n",
    "print(f'tree={tree}\\npredicts={predicts}\\naccuray={accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (Machine-Learning)",
   "language": "python",
   "name": "pycharm-b417c3b3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}