{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# C4.5 Algorithm Implement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3: information gain \n",
    "## C4.5: ratio of information gain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# for numeric data (need encoding data to number).\n",
    "class C45():\n",
    "\n",
    "    def __init__(self, x, maxDepth=3, minGain=0.5):\n",
    "        self.__maxDepth = maxDepth\n",
    "        self.__minGain = minGain\n",
    "        self.__depth = 0\n",
    "        self.__tree = []\n",
    "        self.__features = np.arrage(x.shape[1])\n",
    "\n",
    "    # given the y in that dataset , compute the entropy\n",
    "    def _entropy(self, y):\n",
    "        values, counts = np.unique(y, return_counts=True)\n",
    "        total = len(y)\n",
    "        entropy = -np.sum([count / total * np.log2(count / total) for count in counts])\n",
    "        return entropy\n",
    "\n",
    "    # given attribute index and that dataset(x,y), compute conditional entropy\n",
    "    def _conditEntropy(self, attribute, x, y):\n",
    "        values = np.unique(x[:, attribute])\n",
    "        entropy = []\n",
    "        # iterate each value in that attribute\n",
    "        for value in values:\n",
    "            index = np.argwhere(value == x[:, attribute]).reshape(-1)\n",
    "            # how many data points belongs to that attribute value\n",
    "            label = y[index]\n",
    "            weight = len(label) / len(y)\n",
    "            h = self._entropy(label)\n",
    "            attrEntrop = weight * h\n",
    "            entropy.append(attrEntrop)\n",
    "        conditEntropy = np.sum(entropy)\n",
    "        return conditEntropy\n",
    "\n",
    "    def C45Train(self, x, y, ):\n",
    "        # if all data belong to one class, then return\n",
    "        values, counts = np.unique(y, return_counts=True)\n",
    "        # if only a class, return the tree\n",
    "        if len(values) == 1:\n",
    "            # depth and its class\n",
    "            if self.__depth == 0:\n",
    "                #(tree depth, feature, value, class)\n",
    "                self.__tree.append((self.__depth, 'Root Node', None, values.item()))\n",
    "            return self.__tree\n",
    "        # if no features, return, # if it is root, it is impossible the length of feature is 0\n",
    "        elif len(self.__features) == 0 or self.__depth > self.__maxDepth:\n",
    "            return self.__tree\n",
    "        else:\n",
    "            # start building decision tree\n",
    "            if self.__depth == 0:\n",
    "                valMaxCount = values[np.argmax(counts)]\n",
    "                self.__tree.append((self.__depth, 'Root Node', None, valMaxCount))\n",
    "            # 1. compute data entropy\n",
    "            h = self._entropy(y)\n",
    "            # 2. compute conditional entropy of each feature\n",
    "            conEL = []\n",
    "            for feature in self.__features:\n",
    "                conditEntropy = self._conditEntropy(feature, x, y)\n",
    "                conEL.append(conditEntropy)\n",
    "            # 3. compute gain\n",
    "            gain = h - np.array(conEL)\n",
    "            # add this part and change the criterion variable with infGain\n",
    "            #==* c4.5\n",
    "            hfList = []\n",
    "            for feature in self.__features:\n",
    "                hf = self._entropy(x[:, feature])\n",
    "                hfList.append(hf)\n",
    "            infGain = gain / np.array(hfList)\n",
    "            #==\n",
    "            # 4. max gain\n",
    "            maxGain = np.max(infGain)  # changed\n",
    "            # if max gain less than threshold\n",
    "            if maxGain <= self.__minGain:\n",
    "                return self.__tree\n",
    "            # else split the to sub-nodes\n",
    "            else:\n",
    "                # 5. get feature with max gain and set it as standard to split data\n",
    "                maxGainF = self.__features[np.argmax(infGain)]  # changed\n",
    "                # 6. according to the feature values to split dataset\n",
    "                attrValues = np.unique(x[:, maxGainF])\n",
    "                # a little tricky\n",
    "                loop = self.__depth\n",
    "                preXsub, preYsub, subxs, subys = None, None, None, None\n",
    "                for k, attrValue in enumerate(attrValues):\n",
    "                    xAttrIndex = np.argwhere(attrValue == x[:, maxGainF])\n",
    "                    xsub = x[xAttrIndex, :]\n",
    "                    ysub = y[xAttrIndex]\n",
    "                    if k > 0:\n",
    "                        subxs = np.vstack((preXsub, xsub))\n",
    "                        subys = np.vstack((preYsub, ysub))\n",
    "                    preXsub = xsub\n",
    "                    preYsub = ysub\n",
    "                    subvals, subcts = np.unique(ysub, return_counts=True)\n",
    "                    val = subvals[np.argmax(subcts)]\n",
    "                    self.__tree.append((loop + 1, maxGainF, attrValue, val))\n",
    "                # depth + 1 (important: depth increase must be after the loop above)\n",
    "                self.__depth += 1\n",
    "                if self.__depth > self.__maxDepth:\n",
    "                    return self.__tree\n",
    "                else:\n",
    "                    # after built siblings, remove the feature to ready to next loop\n",
    "                    for subX, subY in zip(subxs, subys):\n",
    "                        leftF = np.argwhere(maxGainF != self.__features)\n",
    "                        self.__features = np.array(self.__features)[leftF].tolist()\n",
    "                        return self.C45Train(subX, subY)\n",
    "\n",
    "    # tree structure: (tree depth,feature,value,class)\n",
    "    def predict(self, x):\n",
    "        predL=[]\n",
    "        # for each data points\n",
    "        for data in x:\n",
    "            # a data point has match history in the tree\n",
    "            matchL=[]\n",
    "            for attribute,value in enumerate(data):\n",
    "                # iterative tree\n",
    "                for node in self.__tree:\n",
    "                    # if mach\n",
    "                    if attribute==node[1]:\n",
    "                        if value==node[2]:\n",
    "                            matchL.append((node[0],node[3]))\n",
    "                        else:\n",
    "                            continue\n",
    "                    else:\n",
    "                        continue\n",
    "            pred=max(matchL,key=lambda ele:ele[0])[1]\n",
    "            predL.append(pred)\n",
    "        return predL\n",
    "\n",
    "    def accuracy(self,x,y):\n",
    "        predL=self.predict(x)\n",
    "        return np.mean(y==predL)\n",
    "\n",
    "\n",
    "                    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (Machine-Learning)",
   "language": "python",
   "name": "pycharm-b417c3b3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}